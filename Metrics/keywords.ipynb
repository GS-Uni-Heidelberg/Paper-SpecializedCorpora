{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyness Metrics (for Lemmatized Corpora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.corpus import Corpus, FrequencyCorpus\n",
    "from src.metrics import keyness\n",
    "from src.corpus_creation import document_retriever as dr\n",
    "import pathlib\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from src.load_data import load_files\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data...\n",
    "\n",
    "Put the path to your corpus in the variable `CORPUSDIR`.\n",
    "\n",
    "I assume that the data is a set of json files, each containing a list of lemmata under the key 'lemmas'.\n",
    "If you have a different format, you need to adjust the code accordingly. The result should be a list of lists of lemmata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the path to the directory containing the corpus files here\n",
    "CORPUSDIR = '/home//.../full'\n",
    "\n",
    "docs, metadata = load_files(CORPUSDIR)\n",
    "corpus = Corpus(docs, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create two subcorpora:\n",
    "\n",
    "1. STUDY CORPUS: All documents containing the word 'KI' or the 2-gram 'künstlich Intelligenz' at least once (You could of course also use different search terms or increase the number of hits needed to include a document.)\n",
    "2. REFERENCE CORPUS: All other documents. (Here, you could also use a completely different corpus, e.g. Leipzig Corpora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = dr.match_wordlist(\n",
    "    corpus, ['KI', ('künstlich', 'Intelligenz'),], min=1\n",
    ")\n",
    "\n",
    "study_corpus = dr.corpus_from_found(\n",
    "    hits, source_corpus=corpus,\n",
    "    goal_corpus='FrequencyCorpus'\n",
    ")\n",
    "reference_corpus = dr.corpus_from_notfound(\n",
    "    hits, source_corpus=corpus,\n",
    "    goal_corpus='FrequencyCorpus'\n",
    ")\n",
    "# with open('/home/.../Data/Reference/Leipzig-Corpora/reference_corpus_20-25.json', 'r') as f:\n",
    "#     reference_corpus = json.load(f)\n",
    "#     reference_corpus = random.sample(reference_corpus, 30000)\n",
    "# reference_corpus = FrequencyCorpus(reference_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for every word in our study corpus, we calculate its keyness score. You can use different metrics here:\n",
    "\n",
    "1. Statistical Significance:\n",
    "    + Log-Likelihood (according to Chi-Square)\n",
    "    + Log-Ratio (according to Rayson)\n",
    "2. Effect size:\n",
    "    + Odds Ratio\n",
    "    + Percentage Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keynesses = keyness.keyword_list(\n",
    "    study_corpus, reference_corpus,\n",
    "    metric='percent_difference',\n",
    "    min_docs=3,\n",
    "    smoothing=0.5,\n",
    "    max_ngram_len=1,\n",
    "    filter_stopwords=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the results with a minimum keyness and take only the top n results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.corpus_creation import handle_wordlists\n",
    "\n",
    "filtered_df = handle_wordlists.remove_redundant(\n",
    "    keynesses\n",
    ")\n",
    "filtered_df = filtered_df[filtered_df['Keyness'] > 2.0]\n",
    "\n",
    "\n",
    "# top-50\n",
    "filtered_df = filtered_df.head(50)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the list of keywords to create our thematic corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_docs = dr.match_wordlist(\n",
    "    corpus, filtered_df['Word'].tolist(), min=1\n",
    ")\n",
    "created_corpus = dr.corpus_from_found(\n",
    "    found_docs, source_corpus=corpus,\n",
    "    goal_corpus='Corpus'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
