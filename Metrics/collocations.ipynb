{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocation Metrics\n",
    "This code computes collocations for a given corpus.\n",
    "\n",
    "You can also evaluate how these collocations work as a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.corpus import FrequencyCorpus\n",
    "from src.metrics import cooccurrence_parallel as co\n",
    "from src.load_data import load_files\n",
    "from src.corpus_creation import document_retriever as dr\n",
    "from src.corpus_creation import handle_wordlists as hw\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data...\n",
    "\n",
    "Put the path to your corpus in the variable `CORPUSDIR`.\n",
    "\n",
    "I assume that the data is a set of json files, each containing a list of lemmata under the key 'lemmas'.\n",
    "If you have a different format, you need to adjust the code accordingly. The result should be a list of lists of lemmata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the path to the directory containing the corpus files here\n",
    "CORPUSDIR = '/home//.../final_corpus'\n",
    "\n",
    "data, metadata = load_files(CORPUSDIR)\n",
    "corpus = FrequencyCorpus(data, metadata, filter=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculcate the collocations of the unigram 'KI' and the bigram 'k端nstliche Intelligenz' as if they were one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating 'k端nstlich Intelligenz' as one token and giving it the name 'KI'\n",
    "# Now, 'k端nstlich Intelligenz' and 'KI' are the same token\n",
    "corpus.treat_as_one(['k端nstlich', 'Intelligenz'], 'KI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method `count_cooccurrences` to get a cooccurrence matrix for the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrences = co.Cooccurrences(window_size=None, unit_separator='\\n\\n', duplicate_counting=True)\n",
    "cooccurrences.count_cooccurrences(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the matrix, we can calculate collocation metrics for the unigram/bigram combo 'KI'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = co.all_collocations(\n",
    "    cooccurrences,\n",
    "    'KI',\n",
    "    co.calculate_pmi,\n",
    "    min_count=1,\n",
    "    smoothing=0.0001,\n",
    "    normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's filter the resulting dataframe to only include positive values and add a row with document counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unigrams so that we get the document frequencies\n",
    "_ = corpus.get_unigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['Stat'] > 0]\n",
    "filtered_df['Doc_Freq'] = filtered_df['Term'].apply(lambda term: corpus.ngram_doccounts[1].get((term,), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = 'wordlists/collocations/windowsizeParagraph-npmi.csv'\n",
    "if os.path.exists(FILEPATH):\n",
    "    raise FileExistsError(\n",
    "        f\"File {FILEPATH} already exists. Please remove it or choose a different name.\"\n",
    "    )\n",
    "filtered_df.to_csv(FILEPATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the collocations as a query...\n",
    "\n",
    "First get the top 50 collocations, then use them to query the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[filtered_df['Doc_Freq'] >= 5]\n",
    "filtered_df = hw.top_x_with_core(50, 'Stat', filtered_df, ['KI'])\n",
    "\n",
    "wordlist = filtered_df['Term'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_to_try = [1, 3, 5]\n",
    "for n in min_to_try:\n",
    "    print(f\"+++++ MIN == {n} +++++\")\n",
    "    hits = dr.match_wordlist(\n",
    "        corpus,\n",
    "        wordlist=wordlist,\n",
    "        min=n,\n",
    "        unique=False\n",
    "    )\n",
    "    _ = dr.eval_retrieval(\n",
    "        corpus,\n",
    "        hits,\n",
    "        annotator='gold_label',\n",
    "        mode='pooling'\n",
    "    )\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
