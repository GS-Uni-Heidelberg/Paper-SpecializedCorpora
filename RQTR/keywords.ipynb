{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyness Metrics (for Lemmatized Corpora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.corpus import Corpus, FrequencyCorpus\n",
    "from src.metrics import keyness\n",
    "from src.corpus_creation import document_retriever as dr\n",
    "import pathlib\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data...\n",
    "\n",
    "Put the path to your corpus in the variable `CORPUSDIR`.\n",
    "\n",
    "I assume that the data is a set of json files, each containing a list of lemmata under the key 'lemmas'.\n",
    "If you have a different format, you need to adjust the code accordingly. The result should be a list of lists of lemmata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the path to the directory containing the corpus files here\n",
    "CORPUSDIR = '/home/brunobrocai/Data/MoWiKo/Paper-themKorp/full'\n",
    "\n",
    "files = pathlib.Path(CORPUSDIR).iterdir()\n",
    "docs = []\n",
    "metadata = []\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        doc = json.load(f)\n",
    "        docs.append(doc['lemmas'])\n",
    "        metadata.append({'h1': doc['h1'], 'url': doc['url']})\n",
    "corpus = Corpus(docs, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create two subcorpora:\n",
    "\n",
    "1. STUDY CORPUS: All documents containing the word 'KI' or the 2-gram 'künstlich Intelligenz' at least once (You could of course also use different search terms or increase the number of hits needed to include a document.)\n",
    "2. REFERENCE CORPUS: All other documents. (Here, you could also use a completely different corpus, e.g. Leipzig Corpora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = dr.match_wordlist(\n",
    "    corpus, ['KI', ('künstlich', 'Intelligenz'),], min=1\n",
    ")\n",
    "\n",
    "study_corpus = dr.corpus_from_found(\n",
    "    hits, source_corpus=corpus,\n",
    "    goal_corpus='FrequencyCorpus'\n",
    ")\n",
    "reference_corpus = dr.corpus_from_notfound(\n",
    "    hits, source_corpus=corpus,\n",
    "    goal_corpus='FrequencyCorpus'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for every word in our study corpus, we calculate its keyness score. You can use different metrics here:\n",
    "\n",
    "1. Statistical Significance:\n",
    "    + Log-Likelihood (according to Chi-Square)\n",
    "    + Log-Ratio (according to Rayson)\n",
    "2. Effect size:\n",
    "    + Odds Ratio\n",
    "    + Percentage Difference\n",
    "\n",
    "More to come...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keynesses = {}\n",
    "for word in study_corpus.get_unigrams():\n",
    "    contingency_table = keyness.corpus_to_contingency(\n",
    "        word, study_corpus, reference_corpus\n",
    "    )\n",
    "    keynesses[word] = keyness.log_likelihood_scipy(contingency_table)\n",
    "\n",
    "df = pd.DataFrame(keynesses.items(), columns=['Word', 'LL'])\n",
    "df = df.sort_values(by='LL', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the results with a minimum keyness and take only the top n results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['LL'] > 2.0]\n",
    "\n",
    "# top-50\n",
    "filtered_df = filtered_df.head(50)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the list of keywords to create our thematic corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_docs = dr.match_wordlist(\n",
    "    corpus, filtered_df['Word'].tolist(), min=2\n",
    ")\n",
    "created_corpus = dr.corpus_from_found(\n",
    "    found_docs, source_corpus=corpus,\n",
    "    goal_corpus='Corpus'\n",
    ")\n",
    "\n",
    "for _, meta in created_corpus:\n",
    "    print(meta['h1'])\n",
    "    print(meta['url'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
