{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eaf3efe",
   "metadata": {},
   "source": [
    "# Combine RQTR and Keyness Metrics\n",
    "\n",
    "With this notebook, you can create a search query by combining the RQTR methods and keyword methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import necessary libraries\n",
    "\n",
    "from src.corpus import Corpus, FrequencyCorpus\n",
    "from src.metrics import keyness\n",
    "from src.corpus_creation import document_retriever as dr\n",
    "import pathlib\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from src.load_data import load_files, load_reference_sample\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc731d8a",
   "metadata": {},
   "source": [
    "Loading the data...\n",
    "\n",
    "Put the path to your corpus in the variable `CORPUSDIR`.\n",
    "\n",
    "I assume that the data is a set of json files, each containing a list of lemmata under the key 'lemmas'. With this format, the `load_files` function will create two lists that can be turned into a corpus.\n",
    "\n",
    "For corpora that are inside of a single file and lack metadata and other features (i.e. our reference corpus format), you can use the `load_reference_sample` function.\n",
    "\n",
    "If you have a different format, you need to adjust the code accordingly. The result should be a list of lists of lemmata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the path to the directory containing the corpus files here\n",
    "CORPUSDIR = '/home/brunobrocai/Data/MoWiKo/Paper-themKorp/full'\n",
    "\n",
    "docs, metadata = load_files(CORPUSDIR)\n",
    "corpus = Corpus(docs, metadata)\n",
    "\n",
    "# While we're at it, we can also load the reference corpora\n",
    "REFERENCE_CORPUS = '/home/brunobrocai/Data/Reference/Leipzig-Corpora/reference_corpus_20-25.json'\n",
    "reference_docs = load_reference_sample(REFERENCE_CORPUS)\n",
    "reference_corpus_leipzig = FrequencyCorpus(reference_docs)\n",
    "\n",
    "reference_corpus_scraped = FrequencyCorpus(docs, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051849fc",
   "metadata": {},
   "source": [
    "## 1: Keyness calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff4a9e",
   "metadata": {},
   "source": [
    "Using the corpus we just loaded, we can create a subcorpus by selecting files that contain terms from the `SEARCH_TERMS` list.\n",
    "\n",
    ">The `match_wordlist` function has additional parameters to customize your search. For example, you can:\n",
    ">1. Edit the min parameter to set the minimum number of matches required for a file to be included in the subcorpus.\n",
    ">2. Set *unique* to True to only count unique matches (i.e. if a file contains the same term multiple times, it will only be counted once).\n",
    "\n",
    "From our search term matches, we can load two types of corpora: ones that *do match* and ones that *do not match* (i.e. all other documents). We can use these as study and reference corpora, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d843221",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_TERMS = ['KI', ('künstlich', 'Intelligenz')]\n",
    "\n",
    "# Find the documents that contain the search terms (at least min times)\n",
    "hits = dr.match_wordlist(\n",
    "    corpus, SEARCH_TERMS, min=1\n",
    ")\n",
    "\n",
    "# Load the found documents into a new corpus\n",
    "study_corpus = dr.corpus_from_found(\n",
    "    hits, source_corpus=corpus,\n",
    "    goal_corpus=FrequencyCorpus\n",
    ")\n",
    "\n",
    "# We can also create a corpus from the documents that do not contain the search terms\n",
    "reference_corpus_nomatch = dr.corpus_from_notfound(\n",
    "    hits, source_corpus=corpus,\n",
    "    goal_corpus=FrequencyCorpus\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74497879",
   "metadata": {},
   "source": [
    "Let's actually perform the **keyness calculation**! We can use different functions here, either `keyword_list` or `keyword_list_ngram`. The first one calculates keyness for single words *and* ngrams. You can specify up to which length you want to calculate ngrams with the *max_ngram_len* parameter. The second one only calculates keyness for *one given ngram length* (use length 1 for words).\n",
    "\n",
    "As function parameters, you must specify study and reference corpus. Both must be FrequencyCorpus python objects. In addition, you can specify the following:\n",
    "1. **metric**: Which keyness metric to use. 'log_likelihood_rayson' is commonly used in corpus linguistics. Alternatively, use 'odds_ratio' or 'percent_difference' for effect size metrics. There are even more metrics you can use -- just check the metrics.keyness module!\n",
    "2. **smoothing**: We use Laplace smoothing, not least to avoid division by zero. You can specify the amount of smoothing. In general, the higher the smoothing, the more conservative the results. The default is 0.00001, which is quite low.\n",
    "3. **min_freq**: Only keywords that appear at least this often in the study corpus will be included in the results. The default is 1, which means that all keywords are included.\n",
    "4. **min_docs**: Only keywords that appear in at least this many documents in the study corpus will be included in the results. The default is 1, which means that all keywords are included.\n",
    "5. **filter_stopwords**: If set to True, stopwords and ngrams beginning or ending with stopwords will be filtered out. The default is True.\n",
    "6. **filter**: You can specify a filter function here, e.g. in order to filter out punctuation when counting frequencies. (By default, all words without alphabetic characters are already ignored when loading a corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82670108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a keyword list for ngrams of length 1 and 2\n",
    "\n",
    "keynesses = keyness.keyword_list(\n",
    "    study_corpus=study_corpus,\n",
    "    ref_corpus=reference_corpus_scraped,\n",
    "    metric='log_likelihood_rayson',\n",
    "    max_ngram_len=2,\n",
    "    min_docs=3,\n",
    "    smoothing=0.5,\n",
    "    filter_stopwords=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d51c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the first 50 keywords\n",
    "keynesses.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0091117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also save pandas dataframes (e.g. the keyword list) to a file\n",
    "# keynesses.to_excel('keyword_list.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa1b39",
   "metadata": {},
   "source": [
    "## 2: RQTR calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405bdc0",
   "metadata": {},
   "source": [
    "Now that we have our keyness results, let them be for a file and calculate RQTR. As a first step, copy our corpus from before.\n",
    "\n",
    "Then, we pick the base terms.\n",
    "\n",
    "**Note:** unfortunately, as of now, n-gram base terms are not supported. You need to use the `treat_as_one` method of the corpus to turn an ngram into a single token. This will be changed in the future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ebddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "corpus_copy = deepcopy(corpus)\n",
    "\n",
    "# Picking base terms\n",
    "BASE_TERMS = ('künstlich_Intelligenz', 'KI')\n",
    "\n",
    "# Treating 'künstlich Intelligenz' as one token -- this is a bit of a hack and will be changed in a future update\n",
    "corpus.treat_as_one(['künstlich', 'Intelligenz'], 'künstlich_Intelligenz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ca099",
   "metadata": {},
   "source": [
    "Now calculate the baseline RQTR value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0fcd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import rqtr_lemma\n",
    "\n",
    "baseline, core_term =rqtr_lemma.qtr_baseline(\n",
    "    BASE_TERMS[0], BASE_TERMS[1], corpus\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3b0f6",
   "metadata": {},
   "source": [
    "After calculating the baseline QTR value, we can move on to calculating the QTR (or RQTR or RQTRn) values for all words/ngrams that cooccur with the base terms at least once. As a first step, count these cooccurrences and the instances where they do not cooccur. Like with keywords, you can do this for a specific ngram length (`count_cooccurence_ngram`) or for all ngrams up to a certain length (`count_cooccurence_ngram`).\n",
    "\n",
    "You also have the following parameter to play with:\n",
    "1. **min_count**: How often a word/phrase must cooccur with the base term in order to be included in the results. The default is 1.\n",
    "\n",
    "After that, we can calculate the metric we want by using the `cooccurence_to_metric` function. You pass the cooccurrence results and the baseline. Also, specify which metric you want to use (baseline is 'rqtrn')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurence_values = rqtr_lemma.count_cooccurence(\n",
    "    BASE_TERMS,\n",
    "    corpus,\n",
    "    min_count=1,\n",
    "    max_ngram_len=1\n",
    ")\n",
    "rqtrn_table = rqtr_lemma.cooccurence_to_metric(\n",
    "    cooccurence_values,\n",
    "    baseline,\n",
    "    metric='rqtrn'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, you could save the rqtrn table to a file\n",
    "# rqtrn_table.to_excel('rqtrn_table.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f60a8c6",
   "metadata": {},
   "source": [
    "## 3: Combined Evaluation\n",
    "\n",
    "With the RQTR and keyness results, we can now create a final search term query with Pandas magic!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f987d",
   "metadata": {},
   "source": [
    "Combine the two dataframes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea08ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with both keyness and rqtrn\n",
    "\n",
    "combined_df = pd.merge(\n",
    "    rqtrn_table,\n",
    "    keynesses,\n",
    "    on='Word',\n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17cfc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yet again, we can save it...\n",
    "# combined_df.to_excel('Combined-RQTRn-Keyness.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ccb0a",
   "metadata": {},
   "source": [
    "We can now use pandas dataframe methods to filter the results. For example, we can say we can keep only results above a certain value threshold.\n",
    "\n",
    "See below for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that lack rqtrn or keyness values\n",
    "filtered_df = combined_df.dropna()\n",
    "\n",
    "# Filter the dataframe to only include rows with rqtrn > 0 and keyness > 10\n",
    "filtered_df = filtered_df[\n",
    "    (filtered_df['Keyness'] > 10) &\n",
    "    (filtered_df['RQTRN'] > 0)\n",
    "]\n",
    "\n",
    "# Keep only the top 50 rows after sorting by rqtrn\n",
    "filtered_df = filtered_df.sort_values(\n",
    "    by='RQTRN', ascending=False\n",
    ").head(50)\n",
    "\n",
    "# Let's take a look at the filtered dataframe\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d5e35",
   "metadata": {},
   "source": [
    "Now that we have our final dataframe, we can use the 'Word' row as a search query to create our final corpus.\n",
    "\n",
    "The function we use will be familiar: it is the `match_wordlist` function we used before. Again, we have the parameters *min* and *unique* to play with.\n",
    "Then we (again) use `corpus_from_found` to create our final thematic corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d54d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_docs = dr.match_wordlist(\n",
    "    corpus,\n",
    "    wordlist=combined_df['Word'].tolist(),\n",
    "    min=5  # Let's be strict\n",
    ")\n",
    "created_corpus = dr.corpus_from_found(\n",
    "    found_docs,\n",
    "    source_corpus=corpus,\n",
    "    goal_corpus='Corpus'\n",
    ")\n",
    "\n",
    "# YOU can uncomment the following lines to print the metadata of the created corpus\n",
    "# for _, meta in created_corpus:\n",
    "#     print(meta['h1'])\n",
    "#     print(meta['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e78e9",
   "metadata": {},
   "source": [
    "If the corpus we loaded is annotated, we can evaluate how well our search query and search parameters performed.\n",
    "\n",
    "The evaluation parameters are:\n",
    "1. **annotator**: The key under which the annotations are stored. Use:\n",
    "   1. *expert_annotator_1* for Janine\n",
    "   2. *expert_annotator_2* for Bruno\n",
    "   3. *majority_vote* for an average of all expert and HiWi annotations\n",
    "2. **mode**:\n",
    "   1. *'pooling'*: treat all non-annotated files as negative examples\n",
    "   2. *'annotated'* only evaluate files that are annotated, ignore the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f9b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.eval_retrieval(\n",
    "    corpus,\n",
    "    found_docs,\n",
    "    annotator='majority_vote',\n",
    "    mode='pooling'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6ad0f",
   "metadata": {},
   "source": [
    "You can also create an annotation corpus from the found documents. Specify which annotator you want to look at to see which files are already annotated (if you take 'majority_vote', then you will get all files that at least one person annotated).\n",
    "\n",
    "Also specify the path you want to copy the files into. The files will be readable txt files with metadata for easy annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f320ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.prepare_annotations(\n",
    "    corpus,\n",
    "    found_docs,\n",
    "    annotator='majority_vote',  # Ignore everything that was annotated at least once\n",
    "    goalpath='annotation_round2'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
