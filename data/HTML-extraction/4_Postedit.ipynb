{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit the corpus post-extraction and post-deduplication\n",
    "The reasons for this editing are manyfold. After extracting the text, many new errors with some files can become apparent. The text might indicate that what you have is, in fact, not an article, or it might be in a language you do not like.\n",
    "\n",
    "This notebook gives you tools to find and delete such files.\n",
    "\n",
    "The final group of cells also adds some preprocessing with SPACY.\n",
    "\n",
    ">WARNING: DO NOT RUN 'ALL CELLS' IN THIS NOTEBOOK. ONLY RUN THE CELLS YOU NEED AND HAVE TESTED, OR YOU MIGHT DELETE FILES YOU DID NOT WANT TO DELETE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import postedit_corpus as pc\n",
    "from functions import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOAL_DIR = '....'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to copy the files to a new directory before running this notebook, so you can always go back to the original files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.utils import copy_jsoncorpus\n",
    "\n",
    "copy_jsoncorpus(GOAL_DIR, f'{GOAL_DIR}-copy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError('Are you ready to proceed?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suspicios Text Lengths\n",
    "Some texts in your corpus might be very short. This can be an indication that the text is not an article, or, if it is, that it is cut off or otherwise non-useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = pc.get_textlens(GOAL_DIR, 'text_deduped')\n",
    "pc.plot_textlens(GOAL_DIR)\n",
    "pc.print_textlen_stats(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.print_textlen_threshold(GOAL_DIR, 1200, below_threshold=True, text_key='text_deduped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.remove_textlen_threshold(GOAL_DIR, 1200, below_threshold=True, force=True, text_key='text_deduped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unwanted Languages\n",
    "If you are only interested in German texts, you might want to delete all texts that are not in German.\n",
    "We use langdetect to detect the language of the text. This is not perfect, but it is a great start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = pc.plot_languages(GOAL_DIR, text_key='text_deduped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.print_languages(GOAL_DIR, ['en'], languages, text_key='text_deduped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.remove_languages(\n",
    "    GOAL_DIR, ['en', 'tr'],\n",
    "    languages,\n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Paragraph Lengths\n",
    "\n",
    "The average paragraph length can be another indicator for non-article texts. If the average paragraph length is very short, this might be an indication that the text is a list or a table.\n",
    "If it is very long, perhaps something went wrong with the extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plot_avg_parlens(GOAL_DIR, text_key='text_deduped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.print_parlen_threshold(GOAL_DIR, 200, below_threshold=True, text_key='text_deduped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.remove_avg_parlen_threshold(GOAL_DIR, 200, below_threshold=True, force=True, text_key='text_deduped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Suspicions\n",
    "\n",
    "You might have other ideas about what unwanted data looks like in your corpus. Test out some functions and see if you find anything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import functions._postedit_checkers as postcheck\n",
    "\n",
    "\n",
    "# For multi-page articles, keep only 'Komplettansicht' URLs and drop individual pages.\n",
    "def custom_checker(data):\n",
    "    url = data['url']\n",
    "    if re.search(r'/seite-[0-9]+', url):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.print_custom_removal(\n",
    "    GOAL_DIR, custom_checker, 'url'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.apply_custom_removal(\n",
    "    GOAL_DIR, postcheck.zeit_dpa, force=True, title_key='h1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Linguistic Information\n",
    "\n",
    "You can run a spacy tokenizer over your data so that you only have to do it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.add_lemma_token(\n",
    "    GOAL_DIR, text_key='text_deduped', spacy_model='de_core_news_lg'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.utils import rename_files_with_padded_index_prefixed\n",
    "\n",
    "rename_files_with_padded_index_prefixed(GOAL_DIR, 'infoakt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
